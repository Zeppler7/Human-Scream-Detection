{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7086984,"sourceType":"datasetVersion","datasetId":4083138},{"sourceId":154284033,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\ndef count_files_in_directory(directory):\n    return len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-19T06:34:56.167286Z","iopub.execute_input":"2023-12-19T06:34:56.167861Z","iopub.status.idle":"2023-12-19T06:34:56.177940Z","shell.execute_reply.started":"2023-12-19T06:34:56.167834Z","shell.execute_reply":"2023-12-19T06:34:56.176993Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Check input file","metadata":{}},{"cell_type":"code","source":"directory_path = '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/not/'\nfile_count = count_files_in_directory(directory_path)\nprint(f\"Number of files in '{directory_path}': {file_count}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:34:56.179569Z","iopub.execute_input":"2023-12-19T06:34:56.179828Z","iopub.status.idle":"2023-12-19T06:35:00.688909Z","shell.execute_reply.started":"2023-12-19T06:34:56.179796Z","shell.execute_reply":"2023-12-19T06:35:00.687829Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Number of files in '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/not/': 2629\n","output_type":"stream"}]},{"cell_type":"code","source":"directory_path = '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/scream/'\nfile_count = count_files_in_directory(directory_path)\nprint(f\"Number of files in '{directory_path}': {file_count}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:00.690064Z","iopub.execute_input":"2023-12-19T06:35:00.690413Z","iopub.status.idle":"2023-12-19T06:35:00.933675Z","shell.execute_reply.started":"2023-12-19T06:35:00.690386Z","shell.execute_reply":"2023-12-19T06:35:00.932636Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of files in '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/scream/': 908\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# We import the needpackage","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:00.937740Z","iopub.execute_input":"2023-12-19T06:35:00.938412Z","iopub.status.idle":"2023-12-19T06:35:04.742197Z","shell.execute_reply.started":"2023-12-19T06:35:00.938386Z","shell.execute_reply":"2023-12-19T06:35:04.741378Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Now we make the dataloader","metadata":{}},{"cell_type":"markdown","source":"## Define datalo0ader by folder","metadata":{}},{"cell_type":"code","source":"data_path = '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/' #looking in subfolder train\n\nscream_dataset = datasets.ImageFolder(\n    root=data_path,\n    transform=transforms.Compose([transforms.Resize((64,862)),\n                                  transforms.ToTensor()])\n)\n\nlen(scream_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:04.743334Z","iopub.execute_input":"2023-12-19T06:35:04.743789Z","iopub.status.idle":"2023-12-19T06:35:05.147153Z","shell.execute_reply.started":"2023-12-19T06:35:04.743756Z","shell.execute_reply":"2023-12-19T06:35:05.146301Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"3537"},"metadata":{}}]},{"cell_type":"markdown","source":"## Check label meaning","metadata":{}},{"cell_type":"code","source":"class_map=scream_dataset.class_to_idx\n\nprint(\"\\nClass category and index of the images: {}\\n\".format(class_map))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:05.148631Z","iopub.execute_input":"2023-12-19T06:35:05.148971Z","iopub.status.idle":"2023-12-19T06:35:05.154080Z","shell.execute_reply.started":"2023-12-19T06:35:05.148939Z","shell.execute_reply":"2023-12-19T06:35:05.153234Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\nClass category and index of the images: {'not': 0, 'scream': 1}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define trainset and testset","metadata":{}},{"cell_type":"code","source":"#split data to test and train\ntrain_size = int(0.8 * len(scream_dataset)) # We use 80% as train\ntest_size = len(scream_dataset) - train_size\nscream_train_dataset, scream_test_dataset = torch.utils.data.random_split(scream_dataset, [train_size, test_size])\n\nprint(\"Training size:\", len(scream_train_dataset))\nprint(\"Testing size:\",len(scream_test_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:05.155381Z","iopub.execute_input":"2023-12-19T06:35:05.155652Z","iopub.status.idle":"2023-12-19T06:35:05.188975Z","shell.execute_reply.started":"2023-12-19T06:35:05.155629Z","shell.execute_reply":"2023-12-19T06:35:05.188165Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training size: 2829\nTesting size: 708\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Check the sample count in trainset","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# labels in training set\ntrain_classes = [label for _, label in scream_train_dataset]\nCounter(train_classes)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:05.189970Z","iopub.execute_input":"2023-12-19T06:35:05.190237Z","iopub.status.idle":"2023-12-19T06:35:29.430614Z","shell.execute_reply.started":"2023-12-19T06:35:05.190214Z","shell.execute_reply":"2023-12-19T06:35:29.429867Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Counter({0: 2099, 1: 730})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Define trainloader and testloader","metadata":{}},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(\n    scream_train_dataset,\n    batch_size=64,\n    num_workers=2,\n    shuffle=True\n)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    scream_test_dataset,\n    batch_size=64,\n    num_workers=2,\n    shuffle=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:29.434729Z","iopub.execute_input":"2023-12-19T06:35:29.434999Z","iopub.status.idle":"2023-12-19T06:35:29.440520Z","shell.execute_reply.started":"2023-12-19T06:35:29.434977Z","shell.execute_reply":"2023-12-19T06:35:29.439669Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## View a example image shape","metadata":{}},{"cell_type":"code","source":"td = train_dataloader.dataset[0][0]\ntd.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:29.441668Z","iopub.execute_input":"2023-12-19T06:35:29.441982Z","iopub.status.idle":"2023-12-19T06:35:29.458147Z","shell.execute_reply.started":"2023-12-19T06:35:29.441952Z","shell.execute_reply":"2023-12-19T06:35:29.457171Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 64, 862])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Import model","metadata":{}},{"cell_type":"markdown","source":"We use resnbet34 for example","metadata":{}},{"cell_type":"code","source":"from torchvision.models import resnet34\nimport torch\nimport torch.nn as nn\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using {device} device')\n\n# Updated model loading with weights\nmodel = resnet34()\n\n# Updating the fully connected layer and the first convolutional layer\nmodel.fc = nn.Linear(512, 2)\nmodel.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:29.459797Z","iopub.execute_input":"2023-12-19T06:35:29.460070Z","iopub.status.idle":"2023-12-19T06:35:33.091860Z","shell.execute_reply.started":"2023-12-19T06:35:29.460047Z","shell.execute_reply":"2023-12-19T06:35:33.090890Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define loss function and optimizer","metadata":{}},{"cell_type":"code","source":"# cost function used to determine best parameters\ncost = torch.nn.CrossEntropyLoss()\n\n# used to create optimal parameters\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:33.093060Z","iopub.execute_input":"2023-12-19T06:35:33.093374Z","iopub.status.idle":"2023-12-19T06:35:33.099551Z","shell.execute_reply.started":"2023-12-19T06:35:33.093348Z","shell.execute_reply":"2023-12-19T06:35:33.098402Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Define Train and Test function","metadata":{}},{"cell_type":"code","source":"# Create the training function\n\ndef train(dataloader, model, loss, optimizer):\n    model.train()\n    size = len(dataloader.dataset)\n    correct = 0  # Counter for correct predictions\n    total = 0  # Counter for total examples\n\n    for batch, (X, Y) in enumerate(dataloader):\n\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        pred = model(X)\n        loss = cost(pred, Y)\n        loss.backward()\n        optimizer.step()\n\n        # Compute accuracy\n        _, predicted = pred.max(1)\n        total += Y.size(0)\n        correct += predicted.eq(Y).sum().item()\n\n        if batch % 10 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]  Train Accuracy: {(100 * correct / total):.2f}%')\n\n\n# Create the validation/test function\n\ndef test(dataloader, model):\n    size = len(dataloader.dataset)\n    model.eval()\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for batch, (X, Y) in enumerate(dataloader):\n            X, Y = X.to(device), Y.to(device)\n            pred = model(X)\n\n            test_loss += cost(pred, Y).item()\n            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n\n    test_loss /= size\n    correct /= size\n\n    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n    return test_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:33.100830Z","iopub.execute_input":"2023-12-19T06:35:33.101136Z","iopub.status.idle":"2023-12-19T06:35:33.113295Z","shell.execute_reply.started":"2023-12-19T06:35:33.101084Z","shell.execute_reply":"2023-12-19T06:35:33.112323Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"# Define the early stopping parameters\nearly_stopping_patience = 15  # Number of epochs to wait before early stopping\nbest_loss = torch.inf\nwait = 0  # Counter for patience\nepochs = 150\n\nbest_model_weights = None\n\n# Training loop\nfor t in range(epochs):\n    print(f'Epoch {t + 1}\\n-------------------------------')\n    train(train_dataloader, model, cost, optimizer)\n    test_loss = test(test_dataloader, model)\n\n    # Check if the test loss has improved\n    if test_loss < best_loss:\n        best_loss = test_loss\n        wait = 0  # Reset patience\n\n        # Save the best model weights\n        best_model_weights = model.state_dict()\n    else:\n        wait += 1\n\n    if wait >= early_stopping_patience:\n        print(\"Early stopping triggered. No improvement in test loss for {} epochs.\".format(early_stopping_patience))\n        break  # Stop training\n\n# Restore the best model weights\nif best_model_weights is not None:\n    model.load_state_dict(best_model_weights)\n\nprint('Done!')","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:35:33.114450Z","iopub.execute_input":"2023-12-19T06:35:33.114784Z","iopub.status.idle":"2023-12-19T06:43:19.844934Z","shell.execute_reply.started":"2023-12-19T06:35:33.114748Z","shell.execute_reply":"2023-12-19T06:43:19.843760Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nloss: 1.300108  [    0/ 2829]  Train Accuracy: 26.56%\nloss: 0.555992  [  640/ 2829]  Train Accuracy: 69.74%\nloss: 0.485033  [ 1280/ 2829]  Train Accuracy: 75.67%\nloss: 0.380712  [ 1920/ 2829]  Train Accuracy: 77.32%\nloss: 0.491013  [ 2560/ 2829]  Train Accuracy: 77.74%\n\nTest Error:\nacc: 83.5%, avg loss: 0.007317\n\nEpoch 2\n-------------------------------\nloss: 0.345843  [    0/ 2829]  Train Accuracy: 82.81%\nloss: 0.554268  [  640/ 2829]  Train Accuracy: 81.11%\nloss: 0.354082  [ 1280/ 2829]  Train Accuracy: 83.18%\nloss: 0.412931  [ 1920/ 2829]  Train Accuracy: 83.27%\nloss: 0.227363  [ 2560/ 2829]  Train Accuracy: 83.54%\n\nTest Error:\nacc: 82.3%, avg loss: 0.011509\n\nEpoch 3\n-------------------------------\nloss: 0.454768  [    0/ 2829]  Train Accuracy: 85.94%\nloss: 0.442653  [  640/ 2829]  Train Accuracy: 82.39%\nloss: 0.256347  [ 1280/ 2829]  Train Accuracy: 83.93%\nloss: 0.544520  [ 1920/ 2829]  Train Accuracy: 84.83%\nloss: 0.327128  [ 2560/ 2829]  Train Accuracy: 84.72%\n\nTest Error:\nacc: 83.3%, avg loss: 0.007782\n\nEpoch 4\n-------------------------------\nloss: 0.337312  [    0/ 2829]  Train Accuracy: 85.94%\nloss: 0.409676  [  640/ 2829]  Train Accuracy: 86.51%\nloss: 0.348431  [ 1280/ 2829]  Train Accuracy: 86.09%\nloss: 0.380736  [ 1920/ 2829]  Train Accuracy: 85.33%\nloss: 0.359140  [ 2560/ 2829]  Train Accuracy: 85.44%\n\nTest Error:\nacc: 75.6%, avg loss: 0.014060\n\nEpoch 5\n-------------------------------\nloss: 0.223310  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.420741  [  640/ 2829]  Train Accuracy: 85.51%\nloss: 0.315008  [ 1280/ 2829]  Train Accuracy: 87.13%\nloss: 0.262520  [ 1920/ 2829]  Train Accuracy: 87.10%\nloss: 0.367602  [ 2560/ 2829]  Train Accuracy: 87.00%\n\nTest Error:\nacc: 75.3%, avg loss: 0.011864\n\nEpoch 6\n-------------------------------\nloss: 0.286942  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.347516  [  640/ 2829]  Train Accuracy: 84.23%\nloss: 0.301936  [ 1280/ 2829]  Train Accuracy: 86.24%\nloss: 0.269765  [ 1920/ 2829]  Train Accuracy: 86.84%\nloss: 0.278664  [ 2560/ 2829]  Train Accuracy: 86.62%\n\nTest Error:\nacc: 83.1%, avg loss: 0.008406\n\nEpoch 7\n-------------------------------\nloss: 0.253244  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.377065  [  640/ 2829]  Train Accuracy: 87.07%\nloss: 0.200888  [ 1280/ 2829]  Train Accuracy: 87.80%\nloss: 0.317725  [ 1920/ 2829]  Train Accuracy: 88.31%\nloss: 0.261332  [ 2560/ 2829]  Train Accuracy: 88.72%\n\nTest Error:\nacc: 86.4%, avg loss: 0.008854\n\nEpoch 8\n-------------------------------\nloss: 0.265605  [    0/ 2829]  Train Accuracy: 92.19%\nloss: 0.333925  [  640/ 2829]  Train Accuracy: 87.93%\nloss: 0.312786  [ 1280/ 2829]  Train Accuracy: 87.57%\nloss: 0.411843  [ 1920/ 2829]  Train Accuracy: 87.30%\nloss: 0.218785  [ 2560/ 2829]  Train Accuracy: 86.78%\n\nTest Error:\nacc: 80.8%, avg loss: 0.009469\n\nEpoch 9\n-------------------------------\nloss: 0.238833  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.217378  [  640/ 2829]  Train Accuracy: 87.50%\nloss: 0.164011  [ 1280/ 2829]  Train Accuracy: 88.47%\nloss: 0.372218  [ 1920/ 2829]  Train Accuracy: 88.36%\nloss: 0.190431  [ 2560/ 2829]  Train Accuracy: 88.45%\n\nTest Error:\nacc: 83.5%, avg loss: 0.006301\n\nEpoch 10\n-------------------------------\nloss: 0.302885  [    0/ 2829]  Train Accuracy: 92.19%\nloss: 0.446595  [  640/ 2829]  Train Accuracy: 87.22%\nloss: 0.257938  [ 1280/ 2829]  Train Accuracy: 88.02%\nloss: 0.356836  [ 1920/ 2829]  Train Accuracy: 88.46%\nloss: 0.307940  [ 2560/ 2829]  Train Accuracy: 88.30%\n\nTest Error:\nacc: 87.9%, avg loss: 0.005175\n\nEpoch 11\n-------------------------------\nloss: 0.217982  [    0/ 2829]  Train Accuracy: 90.62%\nloss: 0.368707  [  640/ 2829]  Train Accuracy: 91.05%\nloss: 0.229807  [ 1280/ 2829]  Train Accuracy: 89.51%\nloss: 0.129306  [ 1920/ 2829]  Train Accuracy: 89.87%\nloss: 0.145924  [ 2560/ 2829]  Train Accuracy: 89.60%\n\nTest Error:\nacc: 81.2%, avg loss: 0.009907\n\nEpoch 12\n-------------------------------\nloss: 0.331366  [    0/ 2829]  Train Accuracy: 87.50%\nloss: 0.242465  [  640/ 2829]  Train Accuracy: 90.77%\nloss: 0.411576  [ 1280/ 2829]  Train Accuracy: 89.29%\nloss: 0.181031  [ 1920/ 2829]  Train Accuracy: 89.87%\nloss: 0.208250  [ 2560/ 2829]  Train Accuracy: 90.21%\n\nTest Error:\nacc: 83.9%, avg loss: 0.007796\n\nEpoch 13\n-------------------------------\nloss: 0.244847  [    0/ 2829]  Train Accuracy: 85.94%\nloss: 0.264190  [  640/ 2829]  Train Accuracy: 89.35%\nloss: 0.340577  [ 1280/ 2829]  Train Accuracy: 88.54%\nloss: 0.193983  [ 1920/ 2829]  Train Accuracy: 87.45%\nloss: 0.189435  [ 2560/ 2829]  Train Accuracy: 88.26%\n\nTest Error:\nacc: 86.4%, avg loss: 0.005910\n\nEpoch 14\n-------------------------------\nloss: 0.401314  [    0/ 2829]  Train Accuracy: 82.81%\nloss: 0.423481  [  640/ 2829]  Train Accuracy: 86.93%\nloss: 0.221112  [ 1280/ 2829]  Train Accuracy: 88.32%\nloss: 0.215519  [ 1920/ 2829]  Train Accuracy: 88.51%\nloss: 0.196877  [ 2560/ 2829]  Train Accuracy: 89.29%\n\nTest Error:\nacc: 75.4%, avg loss: 0.013021\n\nEpoch 15\n-------------------------------\nloss: 0.318335  [    0/ 2829]  Train Accuracy: 87.50%\nloss: 0.265500  [  640/ 2829]  Train Accuracy: 90.48%\nloss: 0.350545  [ 1280/ 2829]  Train Accuracy: 90.77%\nloss: 0.275121  [ 1920/ 2829]  Train Accuracy: 90.47%\nloss: 0.214657  [ 2560/ 2829]  Train Accuracy: 90.93%\n\nTest Error:\nacc: 85.2%, avg loss: 0.007184\n\nEpoch 16\n-------------------------------\nloss: 0.227780  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.398682  [  640/ 2829]  Train Accuracy: 90.48%\nloss: 0.227337  [ 1280/ 2829]  Train Accuracy: 91.52%\nloss: 0.208961  [ 1920/ 2829]  Train Accuracy: 91.13%\nloss: 0.315490  [ 2560/ 2829]  Train Accuracy: 90.82%\n\nTest Error:\nacc: 85.5%, avg loss: 0.005688\n\nEpoch 17\n-------------------------------\nloss: 0.210366  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.195666  [  640/ 2829]  Train Accuracy: 91.48%\nloss: 0.190016  [ 1280/ 2829]  Train Accuracy: 92.11%\nloss: 0.288052  [ 1920/ 2829]  Train Accuracy: 91.68%\nloss: 0.266265  [ 2560/ 2829]  Train Accuracy: 91.12%\n\nTest Error:\nacc: 80.8%, avg loss: 0.011616\n\nEpoch 18\n-------------------------------\nloss: 0.181201  [    0/ 2829]  Train Accuracy: 92.19%\nloss: 0.230026  [  640/ 2829]  Train Accuracy: 90.06%\nloss: 0.175489  [ 1280/ 2829]  Train Accuracy: 90.77%\nloss: 0.205199  [ 1920/ 2829]  Train Accuracy: 91.13%\nloss: 0.488050  [ 2560/ 2829]  Train Accuracy: 90.51%\n\nTest Error:\nacc: 77.3%, avg loss: 0.011197\n\nEpoch 19\n-------------------------------\nloss: 0.288113  [    0/ 2829]  Train Accuracy: 87.50%\nloss: 0.162914  [  640/ 2829]  Train Accuracy: 92.05%\nloss: 0.151912  [ 1280/ 2829]  Train Accuracy: 90.92%\nloss: 0.323458  [ 1920/ 2829]  Train Accuracy: 91.08%\nloss: 0.109547  [ 2560/ 2829]  Train Accuracy: 91.73%\n\nTest Error:\nacc: 81.9%, avg loss: 0.011142\n\nEpoch 20\n-------------------------------\nloss: 0.094270  [    0/ 2829]  Train Accuracy: 95.31%\nloss: 0.120193  [  640/ 2829]  Train Accuracy: 92.05%\nloss: 0.124164  [ 1280/ 2829]  Train Accuracy: 92.26%\nloss: 0.273145  [ 1920/ 2829]  Train Accuracy: 91.48%\nloss: 0.215739  [ 2560/ 2829]  Train Accuracy: 91.39%\n\nTest Error:\nacc: 83.9%, avg loss: 0.008112\n\nEpoch 21\n-------------------------------\nloss: 0.241529  [    0/ 2829]  Train Accuracy: 89.06%\nloss: 0.190624  [  640/ 2829]  Train Accuracy: 90.34%\nloss: 0.175363  [ 1280/ 2829]  Train Accuracy: 91.96%\nloss: 0.048795  [ 1920/ 2829]  Train Accuracy: 92.14%\nloss: 0.217468  [ 2560/ 2829]  Train Accuracy: 92.61%\n\nTest Error:\nacc: 80.6%, avg loss: 0.010464\n\nEpoch 22\n-------------------------------\nloss: 0.207744  [    0/ 2829]  Train Accuracy: 92.19%\nloss: 0.245714  [  640/ 2829]  Train Accuracy: 90.06%\nloss: 0.241640  [ 1280/ 2829]  Train Accuracy: 90.33%\nloss: 0.294485  [ 1920/ 2829]  Train Accuracy: 90.47%\nloss: 0.258710  [ 2560/ 2829]  Train Accuracy: 90.97%\n\nTest Error:\nacc: 79.0%, avg loss: 0.021257\n\nEpoch 23\n-------------------------------\nloss: 0.174807  [    0/ 2829]  Train Accuracy: 93.75%\nloss: 0.252418  [  640/ 2829]  Train Accuracy: 93.32%\nloss: 0.154993  [ 1280/ 2829]  Train Accuracy: 93.30%\nloss: 0.139110  [ 1920/ 2829]  Train Accuracy: 93.75%\nloss: 0.083918  [ 2560/ 2829]  Train Accuracy: 93.41%\n\nTest Error:\nacc: 81.9%, avg loss: 0.007391\n\nEpoch 24\n-------------------------------\nloss: 0.195327  [    0/ 2829]  Train Accuracy: 93.75%\nloss: 0.174273  [  640/ 2829]  Train Accuracy: 93.61%\nloss: 0.173112  [ 1280/ 2829]  Train Accuracy: 93.68%\nloss: 0.194122  [ 1920/ 2829]  Train Accuracy: 93.45%\nloss: 0.134130  [ 2560/ 2829]  Train Accuracy: 93.14%\n\nTest Error:\nacc: 76.6%, avg loss: 0.023129\n\nEpoch 25\n-------------------------------\nloss: 0.342638  [    0/ 2829]  Train Accuracy: 85.94%\nloss: 0.159997  [  640/ 2829]  Train Accuracy: 92.05%\nloss: 0.238860  [ 1280/ 2829]  Train Accuracy: 92.93%\nloss: 0.181544  [ 1920/ 2829]  Train Accuracy: 92.64%\nloss: 0.182664  [ 2560/ 2829]  Train Accuracy: 92.61%\n\nTest Error:\nacc: 80.9%, avg loss: 0.011665\n\nEarly stopping triggered. No improvement in test loss for 15 epochs.\nDone!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datetime import datetime\n\n# Get the current timestamp in the desired format\ntimestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n\n# Define the file name with the timestamp\nfile_name = f\"Resnet34_Model_{timestamp}.pt\"\n\n# Save the entire model (including architecture and weights)\ntorch.save(model, file_name)\n\n# Print the saved file name\nprint(f\"Model saved as {file_name}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:43:19.846305Z","iopub.execute_input":"2023-12-19T06:43:19.846622Z","iopub.status.idle":"2023-12-19T06:43:20.004729Z","shell.execute_reply.started":"2023-12-19T06:43:19.846593Z","shell.execute_reply":"2023-12-19T06:43:20.003795Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model saved as Resnet34_Model_2023-12-19--06-43-19.pt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Let's see how the model can be used!","metadata":{}},{"cell_type":"markdown","source":"## Define output direction","metadata":{}},{"cell_type":"code","source":"import os\n\n# Directory path\ndirectory = \"/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages\"\n\n# Check if the directory already exists\nif not os.path.exists(directory):\n    # Create the directory\n    os.makedirs(directory)\n    print(f\"Directory '{directory}' created\")\nelse:\n    print(f\"Directory '{directory}' already exists\")\n\n    # Folder names to create\nfolders = [\"Screaming\", \"NotScreaming\"]\n\n# Create each folder\nfor folder in folders:\n    directory_path = os.path.join(directory, folder)\n\n    # Check if the directory already exists\n    if not os.path.exists(directory_path):\n        # Create the directory\n        os.makedirs(directory_path)\n        print(f\"Directory '{directory_path}' created\")\n    else:\n        print(f\"Directory '{directory_path}' already exists\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:43:20.005699Z","iopub.execute_input":"2023-12-19T06:43:20.005973Z","iopub.status.idle":"2023-12-19T06:43:20.013702Z","shell.execute_reply.started":"2023-12-19T06:43:20.005947Z","shell.execute_reply":"2023-12-19T06:43:20.012814Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Directory '/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages' created\nDirectory '/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages/Screaming' created\nDirectory '/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages/NotScreaming' created\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define the function apply to audio","metadata":{}},{"cell_type":"code","source":"import os\nimport torchaudio\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef pad_waveform(waveform, target_length):\n    num_channels, current_length = waveform.shape\n\n    if current_length < target_length:\n        # Calculate the amount of padding needed\n        padding = target_length - current_length\n        # Pad the waveform with zeros on the right side\n        waveform = torch.nn.functional.pad(waveform, (0, padding))\n\n    return waveform\n\n# Define a function to transform audio data into images\ndef transform_data_to_image(audio, sample_rate, label, i):\n    # Pad waveform to a consistent length of 44100 samples\n    audio = pad_waveform(audio, 441000)\n\n    spectrogram_tensor = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64, n_fft=1024)(audio)[0] + 1e-10\n\n    # Save the spectrogram as an image\n    image_path = f'/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages/{label}/audio_img{i}.png'\n\n    plt.imsave(image_path, spectrogram_tensor.log2().numpy(), cmap='viridis')\n    return image_path\n\n# Define the image transformation pipeline\ntransform = transforms.Compose([\n    transforms.Resize((64, 862)),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda x: x[:3, :, :])\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:43:20.014804Z","iopub.execute_input":"2023-12-19T06:43:20.015088Z","iopub.status.idle":"2023-12-19T06:43:20.203914Z","shell.execute_reply.started":"2023-12-19T06:43:20.015063Z","shell.execute_reply":"2023-12-19T06:43:20.203145Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Transform and predict the wav for positive","metadata":{}},{"cell_type":"markdown","source":"Note: Thus we use the train set as example show, you can use any wave audio within 10s","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Define the folder containing WAV files\nfolder_path = '/kaggle/input/human-screaming-detection-dataset/Screaming'  # Replace with the path to your folder\nlabel = 'Screaming'  # Label for the images\n\n# Create an empty list to store data\npredictions_data = []\n\n# Iterate through WAV files in the folder\nfor i, filename in enumerate(os.listdir(folder_path)):\n    if filename.endswith('.wav'):\n        # Load the audio\n        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n\n        # Transform audio to an image and save it\n        image_path = transform_data_to_image(audio, sample_rate, label, i)\n\n        # Load the saved image and apply transformations\n        image = Image.open(image_path)\n        image = transform(image).unsqueeze(0)  # Add batch dimension\n\n        # Make predictions using the model\n        model.eval()\n        with torch.no_grad():\n            outputs = model(image.to(device))\n\n        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n\n        # Store the filename and prediction in the DataFrame\n        predictions_data.append({'Filename': filename, 'Prediction': predict})\n\n# Create a DataFrame from the list of data\nscream_predictions_df = pd.DataFrame(predictions_data)\n\n# Display the DataFrame with predictions\nscream_predictions_df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:43:20.205056Z","iopub.execute_input":"2023-12-19T06:43:20.205346Z","iopub.status.idle":"2023-12-19T06:44:36.557949Z","shell.execute_reply.started":"2023-12-19T06:43:20.205322Z","shell.execute_reply":"2023-12-19T06:44:36.556941Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                Filename  Prediction\n0    nIFbKv1qjfw_out.wav           1\n1    d4v3_z0ISrM_out.wav           0\n2    9AZZncb_yek_out.wav           1\n3    IdenFdkeASo_out.wav           1\n4    LY90s5AgkWM_out.wav           1\n..                   ...         ...\n857  nmbLZtYRoBs_out.wav           1\n858  OtTKt5--3jo_out.wav           0\n859  _zzhHu7HwZc_out.wav           0\n860  ZPAY71_lrEk_out.wav           1\n861  fEgkNrEcwE4_out.wav           0\n\n[862 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nIFbKv1qjfw_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d4v3_z0ISrM_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9AZZncb_yek_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IdenFdkeASo_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LY90s5AgkWM_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>nmbLZtYRoBs_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>OtTKt5--3jo_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>_zzhHu7HwZc_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>ZPAY71_lrEk_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>861</th>\n      <td>fEgkNrEcwE4_out.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>862 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"scream_predictions_df['Prediction'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:44:36.559407Z","iopub.execute_input":"2023-12-19T06:44:36.559973Z","iopub.status.idle":"2023-12-19T06:44:36.575208Z","shell.execute_reply.started":"2023-12-19T06:44:36.559934Z","shell.execute_reply":"2023-12-19T06:44:36.574233Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Prediction\n0    567\n1    295\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Transform and predict the wav for negative","metadata":{}},{"cell_type":"code","source":"# Define the folder containing WAV files\nfolder_path = '/kaggle/input/human-screaming-detection-dataset/NotScreaming'  # Replace with the path to your folder\nlabel = 'NotScreaming'  # Label for the images\nimport pandas as pd\n\n# Create an empty list to store data\npredictions_data = []\n\n# Iterate through WAV files in the folder\nfor i, filename in enumerate(os.listdir(folder_path)):\n    if filename.endswith('.wav'):\n        # Load the audio\n        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n\n        # Transform audio to an image and save it\n        image_path = transform_data_to_image(audio, sample_rate, label, i)\n\n        # Load the saved image and apply transformations\n        image = Image.open(image_path)\n        image = transform(image).unsqueeze(0)  # Add batch dimension\n\n        # Make predictions using the model\n        model.eval()\n        with torch.no_grad():\n            outputs = model(image.to(device))\n\n        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n\n        # Store the filename and prediction in the DataFrame\n        predictions_data.append({'Filename': filename, 'Prediction': predict})\n\n# Create a DataFrame from the list of data\nnot_scream_predictions_df = pd.DataFrame(predictions_data)\n\n# Display the DataFrame with predictions\nnot_scream_predictions_df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:44:36.576425Z","iopub.execute_input":"2023-12-19T06:44:36.576704Z","iopub.status.idle":"2023-12-19T06:48:29.007337Z","shell.execute_reply.started":"2023-12-19T06:44:36.576680Z","shell.execute_reply":"2023-12-19T06:48:29.006377Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                 Filename  Prediction\n0     lcgKlZ1xFGk_out.wav           0\n1     OqXsRZMZQDY_out.wav           0\n2     cRQeAJabx0c_out.wav           0\n3     87IhSY4r2DY_out.wav           0\n4     BI4fjQBZliY_out.wav           0\n...                   ...         ...\n2626  Obc79LFQ05Q_out.wav           0\n2627  bc39DoAFVe0_out.wav           0\n2628  7K4kLUy6Kqo_out.wav           0\n2629  pk0xBpBk0s0_out.wav           0\n2630  XY94GCgWMZM_out.wav           0\n\n[2631 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lcgKlZ1xFGk_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OqXsRZMZQDY_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cRQeAJabx0c_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>87IhSY4r2DY_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BI4fjQBZliY_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2626</th>\n      <td>Obc79LFQ05Q_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2627</th>\n      <td>bc39DoAFVe0_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2628</th>\n      <td>7K4kLUy6Kqo_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2629</th>\n      <td>pk0xBpBk0s0_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2630</th>\n      <td>XY94GCgWMZM_out.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2631 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"not_scream_predictions_df['Prediction'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:48:29.008741Z","iopub.execute_input":"2023-12-19T06:48:29.009147Z","iopub.status.idle":"2023-12-19T06:48:29.017143Z","shell.execute_reply.started":"2023-12-19T06:48:29.009094Z","shell.execute_reply":"2023-12-19T06:48:29.016267Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Prediction\n0    2623\n1       8\nName: count, dtype: int64"},"metadata":{}}]}]}